# Glovo case analysis

### 1) Resource load analysis

- Used websites for evaluation:
  - <https://expandedramblings.com/index.php/glovo/>
  - <https://about.glovoapp.com/press/glovo-delivered-2021-report-highlights-global-delivery-and-consumer-trends/>
  - <https://d3.harvard.edu/platform-digit/submission/meet-glovo-the-app-that-will-deliver-anything-to-your-door/#:~:text=Founded%20in%20Barcelona%20in%202015%2C%20Glovo%20is%20an,to%20you%20%E2%80%9Cwithin%20minutes%E2%80%9D%20%28average%20of%2028%20minutes%29>.

- Shops and restauraunts data.
  - Shops and restauraunts amount for 2021: 130,000.
  - Each shop has approximately 200 positions (including shops, restaurants, etc.). It means that we need to store info about 260,000,000 positions.
  - Lets assume that each position has picture and textual description. Average compressed picture size is 5 MB. Approximately, description in Glovo has 200 symbols, which means 200 B.
  - Also, each position in shop/restaurant can have user rating, from 1 to 5 points and short textual review. Lets say, each shop or restauraunt can have about 200 ratings in average. Review point is 8 B and textual review is about 60 B in average.
  - Total shops and restauraunts data amount: (26000000 x 5000000) + (26000000 x 200) + (26000000 x 68 x 200) = 124 TB

- User data (including regular users, verndors and couriers).
  - Total amount of users: 20,000,000.
  - Average user info weight: 3 KB.
  - Total user data amount: 20000000 x 3000 = 60 GB.

- Data generated by couriers and orders:
  - Average active couriers (2022): 74,000.
  - Number of annual orders (2022): 100,000,000.
  - Average number of daily orders: 100,000,000/365 = 274,000.
  - Average data amount per order: 1 KB.
  - Average delivery duration: 28 minutes (1,560 seconds).
  - Average data amount generated by courier each second: 100 B.
  - Total data generated by couriers per month : 100 x 1560 x 274000 x 30 = 1.28 TB.
  - Total data generated by orders per month: 1000 x 274000 x 30 = 8.2 GB

- Conclusions about storage after resource evaluation:
  - Storage for user info: Relational DB, as not so big data amount.
  - Storage for orders: Relational DB, as ACID required.
  - Storage for shops and restauraunts info: Document-oriented DB for info itself and Blob Storage for pictures.
  - Storage for data generated by couriers: Blob Storage, windows from stream in Parquet files.

### 2) Elements description

- Services:

1. Kinesis Data Stream - streaming service that consumes incoming data from the app and produce it to speed and long-terms storages.
2. Kinesis Firehose - ETL of incoming streaming data. This service will help to apply simple transformation of incoming data and store it in the final storage.
3. S3 - blob storage for long-term storage of data. Data will be stored in batches (windows).
4. Timestream - time-series database service that suits for consumption of incoming streaming data and further analytical purposes.
5. Quicksight - BI service that consumes data in Timestream storage and visualize it for the final user.

- Motivation for Kappa architecture:

1. High reliability
2. High speed
3. No risk of overload

### 3) Functional requirements satisfaction

1. Running tracker: send data to Kinesis Data Stream and further store in S3 and Timestream.
2. Revisit previous run sessions (with visualization): use visualization in Quicksight to get session stats with visualization.
3. Retrieve aggregated stats about working-out sessions for selected time-period: get data from S3 for desired time-window(s).
